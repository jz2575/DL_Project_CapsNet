{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T20:41:44.783266Z",
     "start_time": "2018-04-08T20:41:42.759154Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T20:41:44.875794Z",
     "start_time": "2018-04-08T20:41:44.861464Z"
    }
   },
   "outputs": [],
   "source": [
    "def augmentation(x, max_offset=2):\n",
    "    bz, h, w, c = x.shape\n",
    "    bg = np.zeros([bz, w + 2 * max_offset, h + 2 * max_offset, c])\n",
    "    offsets = np.random.randint(0, 2 * max_offset + 1, 2)\n",
    "    bg[:, offsets[0]:offsets[0] + h, offsets[1]:offsets[1] + w, :] = x\n",
    "    return bg[:, max_offset:max_offset + h, max_offset:max_offset + w, :]\n",
    "\n",
    "\n",
    "def mnist_train_iter(iters=1000, batch_size=32, is_shift_ag=True):\n",
    "    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "    max_offset = int(is_shift_ag) * 2\n",
    "    for i in range(iters):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        images = batch[0].reshape([batch_size, 28, 28, 1])\n",
    "        images = np.concatenate([images] * 3, axis=-1)\n",
    "        yield augmentation(images, max_offset), np.stack(\n",
    "            [batch[1]] * 3, axis=-1)\n",
    "\n",
    "\n",
    "def mnist_test_iter(iters=1000, batch_size=32, is_shift_ag=False):\n",
    "    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "    max_offset = int(is_shift_ag) * 2\n",
    "    for i in range(iters):\n",
    "        batch = mnist.test.next_batch(batch_size)\n",
    "        images = batch[0].reshape([batch_size, 28, 28, 1])\n",
    "        images = np.concatenate([images] * 3, axis=-1)\n",
    "        yield augmentation(images, max_offset), np.stack(\n",
    "            [batch[1]] * 3, axis=-1)\n",
    "\n",
    "\n",
    "def multimnist_train_iter(iters=1000, batch_size=32, is_shift_ag=True):\n",
    "    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "    max_offset = int(is_shift_ag) * 2\n",
    "    for i in range(iters):\n",
    "        batch1 = mnist.train.next_batch(batch_size)\n",
    "        batch2 = mnist.train.next_batch(batch_size)\n",
    "        images1 = augmentation(batch1[0].reshape([batch_size, 28, 28, 1]),\n",
    "                               max_offset)\n",
    "        images2 = augmentation(batch2[0].reshape([batch_size, 28, 28, 1]),\n",
    "                               max_offset)\n",
    "        images = np.logical_or(images1, images2).astype(np.float32)\n",
    "        images = np.concatenate([images, images1, images2], axis=-1)\n",
    "        y1, y2 = batch1[1], batch2[1]\n",
    "        y0 = np.logical_or(y1, y2).astype(np.float32)\n",
    "        yield images, np.stack([y0, y1, y2], axis=-1)\n",
    "\n",
    "\n",
    "def multimnist_test_iter(iters=1000, batch_size=32, is_shift_ag=True):\n",
    "    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "    max_offset = int(is_shift_ag) * 2\n",
    "    for i in range(iters):\n",
    "        batch1 = mnist.test.next_batch(batch_size)\n",
    "        batch2 = mnist.test.next_batch(batch_size)\n",
    "        images1 = augmentation(batch1[0].reshape([batch_size, 28, 28, 1]),\n",
    "                               max_offset)\n",
    "        images2 = augmentation(batch2[0].reshape([batch_size, 28, 28, 1]),\n",
    "                               max_offset)\n",
    "        images = np.logical_or(images1, images2).astype(np.float32)\n",
    "        images = np.concatenate([images, images1, images2], axis=-1)\n",
    "        y1, y2 = batch1[1], batch2[1]\n",
    "        y0 = np.logical_or(y1, y2).astype(np.float32)\n",
    "        yield images, np.stack([y0, y1, y2], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T20:41:44.990362Z",
     "start_time": "2018-04-08T20:41:44.955694Z"
    }
   },
   "outputs": [],
   "source": [
    "class CapsNet(object):\n",
    "    def __init__(self,\n",
    "                 routing_iterations=3,\n",
    "                 batch_size=128,\n",
    "                 is_multi_mnist=False,\n",
    "                 beta1=0.9,\n",
    "                 steps=5000,\n",
    "                 norm=True):\n",
    "        self.iterations = routing_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.is_multi_mnist = float(is_multi_mnist)\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32, [None, 28, 28, 3])\n",
    "        self.h_sample = tf.placeholder(tf.float32, [None, 10, 16])\n",
    "        self.y_sample = tf.placeholder(tf.float32, [None, 10])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 10, 3])\n",
    "\n",
    "        self.norm = norm\n",
    "        self.on_train = tf.placeholder(tf.bool)\n",
    "\n",
    "        x_composed, x_a, x_b = tf.split(self.x, num_or_size_splits=3, axis=3)\n",
    "        y_composed, y_a, y_b = tf.split(self.y, num_or_size_splits=3, axis=2)\n",
    "\n",
    "        valid_mask = self.is_multi_mnist * (tf.reduce_sum(y_composed, axis=[1,2]) - 1.0) \\\n",
    "                      + (1.0 - self.is_multi_mnist) * tf.ones_like(y_composed[:,0,0])\n",
    "\n",
    "        v_digit,reg_term = self.get_CapsNet(x_composed, self.norm, self.on_train)\n",
    "\n",
    "        length_v = tf.reduce_sum(v_digit**2.0, axis=-1)**0.5\n",
    "\n",
    "        x_rec_a = self.get_mlp_decoder(v_digit * y_a)\n",
    "        x_rec_b = self.get_mlp_decoder(v_digit * y_b, reuse=True)\n",
    "        loss_rec_a = tf.reduce_sum((x_rec_a - x_a)**2.0, axis=[1, 2, 3])\n",
    "        loss_rec_b = tf.reduce_sum((x_rec_b - x_b)**2.0, axis=[1, 2, 3])\n",
    "        self.loss_rec = (loss_rec_a + loss_rec_b) / 2.0\n",
    "        self.x_recs = [x_rec_a, x_rec_b]\n",
    "        self.x_sample = self.get_mlp_decoder(\n",
    "            self.h_sample * self.y_sample[:, :, None], reuse=True)\n",
    "        self.loss_cls = tf.reduce_sum(\n",
    "            y_composed[:, :, 0] * tf.maximum(0.0, 0.9 - length_v)**2.0 + 0.5 *\n",
    "            (1.0 - y_composed[:, :, 0]) * tf.maximum(0.0, length_v - 0.1)**2.0,\n",
    "            axis=-1)\n",
    "        self.loss_cls = tf.reduce_sum(\n",
    "            self.loss_cls * valid_mask) / tf.reduce_sum(valid_mask)\n",
    "        self.loss_rec = tf.reduce_sum(\n",
    "            self.loss_rec * valid_mask) / tf.reduce_sum(valid_mask)\n",
    "        self.loss = self.loss_cls + 0.0005 * self.loss_rec + reg_term\n",
    "\n",
    "        global_step = tf.Variable(0)\n",
    "        lr = tf.train.exponential_decay(\n",
    "            0.00075, global_step, steps / 10, 0.96, staircase=False)\n",
    "\n",
    "        self.train = tf.train.AdamOptimizer(\n",
    "            learning_rate=lr, beta1=beta1).minimize(\n",
    "                self.loss, global_step=global_step)\n",
    "\n",
    "        if is_multi_mnist:\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(tf.nn.in_top_k(length_v,tf.argmax(tf.squeeze(y_a), 1),k=2),tf.float32))+\\\n",
    "                            tf.reduce_mean(tf.cast(tf.nn.in_top_k(length_v,tf.argmax(tf.squeeze(y_b), 1),k=2),tf.float32))\n",
    "            self.accuracy /= 2.0\n",
    "        else:\n",
    "            correct_prediction = tf.equal(\n",
    "                tf.argmax(y_composed[:, :, 0], 1), tf.argmax(length_v, 1))\n",
    "            self.accuracy = tf.reduce_mean(\n",
    "                tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def get_CapsNet(self, x, norm, on_train, reuse=False):\n",
    "        with tf.variable_scope('CapsNet', reuse=reuse):\n",
    "            wconv1 = tf.get_variable(\n",
    "                'wconv1', [9, 9, 1, 256],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "            bconv1 = tf.get_variable(\n",
    "                'bconv1', [256],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "            wconv2 = tf.get_variable(\n",
    "                'wconv2', [9, 9, 256, 8 * 32],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "            bconv2 = tf.get_variable(\n",
    "                'bconv2', [8 * 32],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "            wcap = tf.get_variable(\n",
    "                'wcap', [1, 6, 6, 32, 8, 10, 16],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "            b = tf.get_variable(\n",
    "                'coupling_coefficient_logits', [1, 6, 6, 32, 1, 10, 1],\n",
    "                initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        c = tf.stop_gradient(tf.nn.softmax(b, axis=5))\n",
    "        \n",
    "        #L2-regularization\n",
    "        tf.add_to_collection(tf.GraphKeys.WEIGHTS, wconv1)\n",
    "        tf.add_to_collection(tf.GraphKeys.WEIGHTS, wconv2)\n",
    "        tf.add_to_collection(tf.GraphKeys.WEIGHTS, wcap)\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=5.0/50000)\n",
    "        reg_term = tf.contrib.layers.apply_regularization(regularizer)\n",
    "\n",
    "        \n",
    "        if norm:\n",
    "            # BN for the first input\n",
    "            fc_mean, fc_var = tf.nn.moments(\n",
    "                x,\n",
    "                axes=[0, 1, 2],\n",
    "            )\n",
    "            scale = tf.Variable(tf.ones([1]))\n",
    "            shift = tf.Variable(tf.zeros([1]))\n",
    "            epsilon = 0.001\n",
    "            ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "            def mean_var_with_update():\n",
    "                ema_apply_op = ema.apply([fc_mean, fc_var])\n",
    "                with tf.control_dependencies([ema_apply_op]):\n",
    "                    return tf.identity(fc_mean), tf.identity(fc_var)\n",
    "\n",
    "            mean, var = tf.cond(\n",
    "                on_train, mean_var_with_update,\n",
    "                lambda: (ema.average(fc_mean), ema.average(fc_var)))\n",
    "            x = tf.nn.batch_normalization(x, mean, var, shift, scale, epsilon)\n",
    "\n",
    "        conv1 = tf.nn.conv2d(x, wconv1, [1, 1, 1, 1], padding='VALID') + bconv1\n",
    "\n",
    "        if norm:\n",
    "            # BN for the first conv layer\n",
    "            fc_mean, fc_var = tf.nn.moments(\n",
    "                conv1,\n",
    "                axes=[0, 1, 2],\n",
    "            )\n",
    "            scale = tf.Variable(tf.ones([1]))\n",
    "            shift = tf.Variable(tf.zeros([1]))\n",
    "            epsilon = 0.001\n",
    "            ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "            def mean_var_with_update():\n",
    "                ema_apply_op = ema.apply([fc_mean, fc_var])\n",
    "                with tf.control_dependencies([ema_apply_op]):\n",
    "                    return tf.identity(fc_mean), tf.identity(fc_var)\n",
    "\n",
    "            mean, var = tf.cond(\n",
    "                on_train, mean_var_with_update,\n",
    "                lambda: (ema.average(fc_mean), ema.average(fc_var)))\n",
    "            conv1 = tf.nn.batch_normalization(conv1, mean, var, shift, scale,\n",
    "                                              epsilon)\n",
    "\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "        s_primary = tf.nn.conv2d(\n",
    "            conv1, wconv2, [1, 2, 2, 1], padding='VALID') + bconv2\n",
    "\n",
    "        if norm:\n",
    "            # BN for the second conv layer\n",
    "            fc_mean, fc_var = tf.nn.moments(\n",
    "                s_primary,\n",
    "                axes=[0, 1, 2],\n",
    "            )\n",
    "            scale = tf.Variable(tf.ones([1]))\n",
    "            shift = tf.Variable(tf.zeros([1]))\n",
    "            epsilon = 0.001\n",
    "            ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "            def mean_var_with_update():\n",
    "                ema_apply_op = ema.apply([fc_mean, fc_var])\n",
    "                with tf.control_dependencies([ema_apply_op]):\n",
    "                    return tf.identity(fc_mean), tf.identity(fc_var)\n",
    "\n",
    "            mean, var = tf.cond(\n",
    "                on_train, mean_var_with_update,\n",
    "                lambda: (ema.average(fc_mean), ema.average(fc_var)))\n",
    "            s_primary = tf.nn.batch_normalization(s_primary, mean, var, shift,\n",
    "                                                  scale, epsilon)\n",
    "\n",
    "        s_primary = tf.reshape(s_primary, [-1, 6, 6, 32, 8, 1, 1])\n",
    "\n",
    "        v_primary = self.squash(s_primary, axis=4)\n",
    "\n",
    "        #CAPSNET\n",
    "        u = v_primary\n",
    "        u_ = tf.reduce_sum(u * wcap, axis=[4], keepdims=True)\n",
    "        s = tf.reduce_sum(u_ * c, axis=[1, 2, 3], keepdims=True)\n",
    "        v = self.squash(s, axis=-1)\n",
    "\n",
    "        if norm:\n",
    "            # BN for the capsule layer\n",
    "            fc_mean, fc_var = tf.nn.moments(\n",
    "                v,\n",
    "                axes=[0, 1, 2],\n",
    "            )\n",
    "            scale = tf.Variable(tf.ones([1]))\n",
    "            shift = tf.Variable(tf.zeros([1]))\n",
    "            epsilon = 0.001\n",
    "            ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "            def mean_var_with_update():\n",
    "                ema_apply_op = ema.apply([fc_mean, fc_var])\n",
    "                with tf.control_dependencies([ema_apply_op]):\n",
    "                    return tf.identity(fc_mean), tf.identity(fc_var)\n",
    "\n",
    "            mean, var = tf.cond(\n",
    "                on_train, mean_var_with_update,\n",
    "                lambda: (ema.average(fc_mean), ema.average(fc_var)))\n",
    "            v = tf.nn.batch_normalization(v, mean, var, shift, scale, epsilon)\n",
    "\n",
    "        for i in range(self.iterations - 1):\n",
    "            b += tf.reduce_sum(u_ * v, axis=-1, keepdims=True)\n",
    "            c = tf.nn.softmax(b, axis=5)\n",
    "            s = tf.reduce_sum(u_ * c, axis=[1, 2, 3], keepdims=True)\n",
    "            v = self.squash(s, axis=-1)\n",
    "\n",
    "        v_digit = tf.squeeze(v)\n",
    "\n",
    "        return v_digit,reg_term\n",
    "\n",
    "    def get_mlp_decoder(self, h, num_h=[10 * 16, 512, 1024, 784], reuse=False):\n",
    "        h = tf.reshape(h, [-1, 10 * 16])\n",
    "        with tf.variable_scope('decoder', reuse=reuse):\n",
    "            weights = []\n",
    "            for i in range(len(num_h) - 1):\n",
    "                w = tf.get_variable(\n",
    "                    'wfc%d' % i, [num_h[i], num_h[i + 1]],\n",
    "                    initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "                b = tf.get_variable(\n",
    "                    'bfc%d' % i, [num_h[i + 1]],\n",
    "                    initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "                weights.append((w, b))\n",
    "                if i < len(num_h) - 2:\n",
    "                    h = tf.nn.relu(tf.add(tf.matmul(h, w), b))\n",
    "                else:\n",
    "                    h = tf.nn.sigmoid(tf.add(tf.matmul(h, w), b))\n",
    "        x_rec = tf.reshape(h, [-1, 28, 28, 1])\n",
    "        return x_rec\n",
    "\n",
    "    def squash(self, s, axis=-1):\n",
    "        length_s = tf.reduce_sum(s**2.0, axis=axis, keepdims=True)**0.5\n",
    "        v = s * length_s / (1.0 + length_s**2.0)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T20:41:45.069116Z",
     "start_time": "2018-04-08T20:41:45.066309Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "is_multi_mnist = True\n",
    "is_shift_ag = True\n",
    "irun = 0\n",
    "steps = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T20:49:52.818639Z",
     "start_time": "2018-04-08T20:41:45.743483Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0 1.6316246 182.09749 0.1796875 0.3046875\n",
      "100 0.68395054 53.946007 0.59375 0.71875\n",
      "200 0.5422913 50.462147 0.7265625 0.7890625\n",
      "300 0.48848277 51.95821 0.7578125 0.7265625\n",
      "400 0.46840346 44.144913 0.7578125 0.7265625\n",
      "500 0.4508866 47.69269 0.7890625 0.6875\n",
      "600 0.4384737 43.762383 0.7734375 0.765625\n",
      "700 0.45832253 50.35829 0.765625 0.7109375\n",
      "800 0.46756113 47.370953 0.7578125 0.765625\n",
      "900 0.3931062 43.12308 0.78125 0.7421875\n",
      "1000 0.332156 41.22647 0.84375 0.828125\n",
      "1100 0.41959065 42.84593 0.8125 0.7890625\n",
      "1200 0.43307418 45.503494 0.75 0.7109375\n",
      "1300 0.328545 38.79215 0.8359375 0.796875\n",
      "1400 0.3981854 44.534485 0.7734375 0.84375\n",
      "1500 0.3591103 40.442173 0.7734375 0.703125\n",
      "1600 0.35534462 40.75394 0.8359375 0.765625\n",
      "1700 0.29224345 36.971493 0.8984375 0.875\n",
      "1800 0.3655402 44.34655 0.8125 0.8203125\n",
      "1900 0.31456235 40.021187 0.875 0.7109375\n",
      "2000 0.3398939 40.0883 0.828125 0.8828125\n",
      "2100 0.3689578 39.2106 0.828125 0.8125\n",
      "2200 0.30137044 38.021126 0.859375 0.8671875\n",
      "2300 0.32442436 38.933487 0.84375 0.890625\n",
      "2400 0.25795537 37.61683 0.890625 0.8515625\n",
      "2500 0.3007782 37.884544 0.859375 0.8046875\n",
      "2600 0.3718342 37.18816 0.8203125 0.859375\n",
      "2700 0.28050053 37.346504 0.8515625 0.78125\n",
      "2800 0.23587483 38.997395 0.90625 0.8359375\n",
      "2900 0.25832814 37.254654 0.921875 0.8671875\n",
      "3000 0.24035823 35.891518 0.8984375 0.890625\n",
      "3100 0.25831214 38.668068 0.8984375 0.8125\n",
      "3200 0.3445903 39.19223 0.8046875 0.890625\n",
      "3300 0.38401157 40.236008 0.8046875 0.8359375\n",
      "3400 0.34121203 36.214375 0.8125 0.859375\n",
      "3500 0.29220298 38.366604 0.8671875 0.8515625\n",
      "3600 0.2877913 36.1045 0.875 0.875\n",
      "3700 0.30655798 34.727917 0.84375 0.859375\n",
      "3800 0.35180765 38.419796 0.8359375 0.8203125\n",
      "3900 0.34795722 39.343204 0.7890625 0.8359375\n",
      "4000 0.2811441 38.094994 0.875 0.8203125\n",
      "4100 0.31851614 37.15258 0.8671875 0.84375\n",
      "4200 0.23645422 34.51417 0.8984375 0.84375\n",
      "4300 0.27627805 34.872993 0.90625 0.8203125\n",
      "4400 0.29128328 35.856033 0.8828125 0.8125\n",
      "4500 0.32892036 36.270363 0.875 0.8203125\n",
      "4600 0.35944143 38.128227 0.8359375 0.6171875\n",
      "4700 0.25823742 32.827957 0.890625 0.8515625\n",
      "4800 0.272436 34.31417 0.875 0.8359375\n",
      "4900 0.22556658 36.049328 0.921875 0.8203125\n"
     ]
    }
   ],
   "source": [
    "if is_multi_mnist:\n",
    "    train_iter = multimnist_train_iter(\n",
    "        iters=steps, batch_size=batch_size, is_shift_ag=True)\n",
    "    test_iter = multimnist_test_iter(\n",
    "        iters=steps, batch_size=batch_size, is_shift_ag=True)\n",
    "else:\n",
    "    train_iter = mnist_train_iter(\n",
    "        iters=steps, batch_size=batch_size, is_shift_ag=True)\n",
    "    test_iter = mnist_test_iter(\n",
    "        iters=steps, batch_size=batch_size, is_shift_ag=True)\n",
    "\n",
    "net = CapsNet(is_multi_mnist=is_multi_mnist, steps=steps)\n",
    "tf.summary.scalar('error_rate_on_test_set', (1.0 - net.accuracy) * 100.0)\n",
    "tf.summary.scalar('loss_reconstruction_on_test_set', net.loss_rec)\n",
    "merged = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "for X, Y in train_iter:\n",
    "    X_TEST, Y_TEST = next(test_iter)\n",
    "\n",
    "    LS, LS_REC, ACC, _ = sess.run(\n",
    "        [net.loss, net.loss_rec, net.accuracy, net.train],\n",
    "        feed_dict={\n",
    "            net.x: X,\n",
    "            net.y: Y,\n",
    "            net.on_train: True\n",
    "        })\n",
    "    ACC_TEST, result = sess.run(\n",
    "        [net.accuracy, merged],\n",
    "        feed_dict={\n",
    "            net.x: X_TEST,\n",
    "            net.y: Y_TEST,\n",
    "            net.on_train: False\n",
    "        })\n",
    "\n",
    "    if irun % 100 == 0:\n",
    "        print(irun, LS, LS_REC, ACC, ACC_TEST)\n",
    "\n",
    "    irun += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T20:38:49.835590Z",
     "start_time": "2018-04-08T20:38:49.832252Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
